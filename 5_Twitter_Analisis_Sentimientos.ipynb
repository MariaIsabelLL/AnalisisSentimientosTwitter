{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.Twitter Analisis Sentimientos.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+/YV3xOwnu+fGRLbkv4d7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaIsabelLL/AnalisisSentimientosTwitter/blob/main/5_Twitter_Analisis_Sentimientos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR0Ygpc23Yuq",
        "outputId": "75843551-6b2f-44eb-ef4d-9590e425d12c"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "\n",
        "\"\"\" tokenizar tweets\"\"\"\n",
        "def preprocess(s):\n",
        "    emoticons_str = r\"\"\"\n",
        "    (?:\n",
        "        [:=;] # Eyes\n",
        "        [oO\\-]? # Nose (optional)\n",
        "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "    )\"\"\"\n",
        "        \n",
        "    regex_str =[emoticons_str,\n",
        "                r'<[^>]+>' , #HTML tags\n",
        "                r'(?:@[\\w_]+)' , #@-Menci칩n\n",
        "                r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\" , #Hash-tags\n",
        "                r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', #URLs\n",
        "                r'(?:[\\w_]+)' , #Otras Palabras\n",
        "                r'(?:\\S)' #Otras Palabras\n",
        "                ]    \n",
        "    \n",
        "    tokens_re = re.compile (r'('+'|'.join(regex_str)+')' ,re.VERBOSE | re.IGNORECASE)\n",
        "    tokens = tokens_re.findall(s)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bKQ67MnvYVG"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "def bag_of_words(words):\n",
        "    words_dictionary = dict([word, True] for word in words)\n",
        "    #print('dictionario',words_dictionary)\n",
        "    return words_dictionary\n",
        "\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "        stemmed.append(stemmer.stem(item))\n",
        "    return stemmed    \n",
        "\n",
        "def obtain_tokens(tweet):\n",
        "    stemmer = SnowballStemmer('spanish')    \n",
        "    features  = {}\n",
        "    #primero se realiza la identificaci칩n de tokens y se quitan los stopwords\n",
        "    tweet_token = [term for term in preprocess(tweet) if term not in stopwords]\n",
        "    total_words = []\n",
        "    #segundo se obtienen los stemm\n",
        "    for word in stem_tokens(tweet_token,stemmer):\n",
        "        total_words.append(word)\n",
        "    return total_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhMKIIRysnn2"
      },
      "source": [
        "def lee_datos(loc):\n",
        "  listFiles = glob.glob(loc+'*train.xml')\n",
        "  count=0\n",
        "  count1=0\n",
        "  count2=0\n",
        "  pos_reviews = []\n",
        "  neg_reviews = []\n",
        "  pos_reviews_set = []\n",
        "  neg_reviews_set = []\n",
        "\n",
        "  for fileName in listFiles:\n",
        "\n",
        "        soup = BeautifulSoup(open(fileName,'r',encoding='utf8'),features=\"xml\")\n",
        "        for tweet in soup.find_all(\"tweet\"):\n",
        "            words = obtain_tokens(tweet.content.text)\n",
        "            label = tweet.sentiments.polarity.value.text\n",
        "            if (label=='NONE'):\n",
        "                #etiqueta='X'\n",
        "                continue\n",
        "            if (label=='NEU'):\n",
        "                etiqueta='Y'\n",
        "            if (label in ('N','P')):\n",
        "                etiqueta=label\n",
        "                if (label=='N'):\n",
        "                    neg_reviews.append(words)\n",
        "                    count1= count1+1\n",
        "                if (label=='P'):\n",
        "                    pos_reviews.append(words)\n",
        "                    count2= count2+1\n",
        "            count= count+1\n",
        "\n",
        "  for words in pos_reviews:\n",
        "        pos_reviews_set.append((bag_of_words(words), 'P'))\n",
        "  for words in neg_reviews:\n",
        "        neg_reviews_set.append((bag_of_words(words), 'N'))\n",
        "\n",
        "  size = int(len(pos_reviews_set) * 0.1)     \n",
        "  testSet = pos_reviews_set[:size] + neg_reviews_set[:size]\n",
        "  trainSet = pos_reviews_set[size:] + neg_reviews_set[size:]\n",
        "\n",
        "  print('total de casos',count)\n",
        "  print('total de casos positivos',count1)\n",
        "  print('total de casos negativos',count2)\n",
        "  return trainSet, testSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9qBmOCdzf2A",
        "outputId": "f4f62cb8-e666-467b-e1bb-8329fa8c0aaf"
      },
      "source": [
        "#!pip install sklearn-crfsuite\n",
        "import glob\n",
        "import json\n",
        "from sklearn_crfsuite import CRF as CRF_sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn_crfsuite.metrics import flat_f1_score\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "def clasificadorSentimientos(loc):\n",
        "    (trainSet, testSet) = lee_datos(loc)\n",
        "    \n",
        "    #Naive Bayes classifier\n",
        "    classifier1 = nltk.NaiveBayesClassifier.train(trainSet)\n",
        "    print('Naive Bayes classifier',nltk.classify.accuracy(classifier1,  testSet)) \n",
        "\n",
        "    #Predicting on the test set.\n",
        "    X_test = [f for (f,pos) in testSet]\n",
        "    y_test = [pos for (f,pos) in testSet]\n",
        "    predSet=[]\n",
        "    for xtest in X_test:\n",
        "      y_pred = classifier1.classify(xtest)\n",
        "      predSet.append(y_pred)\n",
        "    f1_score2 = flat_f1_score(y_test, predSet, average = 'weighted')\n",
        "    print('f1_score',f1_score2)\n",
        "    report = flat_classification_report(y_test, predSet)\n",
        "    print(report)\n",
        "\n",
        "    return classifier1\n",
        "\n",
        "def prediccionSentimientos(arc,clas):    \n",
        "\n",
        "    with open (arc ,\"r\") as f:  \n",
        "        for line in f:     \n",
        "            if not line.isspace():\n",
        "                tweet = json.loads(line)\n",
        "                #print(\"TWEET:\",tweet[\"text\"])\n",
        "                newTexto = clas.classify(bag_of_words(obtain_tokens(tweet[\"text\"]))) \n",
        "                print(\"Resultado\", newTexto, tweet[\"text\"] )\n",
        "       \n",
        "\n",
        "locCorpusTass1 = '/content/sample_data/tass/'          \n",
        "clas = clasificadorSentimientos(locCorpusTass1)\n",
        "#prediccionSentimientos(\"coronavirus.json\",clas)  \n",
        "tweet1=\"@dw_espanol: Lo m치s triste de la #pandemia del #coronavirus son la cantidad de fallecidos\"\n",
        "tweet2=\"@dw_espanol: Todos los adultos mayores al fin vacunados!!!\"\n",
        "print(tweet1, clas.classify(bag_of_words(obtain_tokens(tweet1)))  )\n",
        "print(tweet2, clas.classify(bag_of_words(obtain_tokens(tweet2)))  )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de casos 5736\n",
            "total de casos positivos 1335\n",
            "total de casos negativos 1232\n",
            "Naive Bayes classifier 0.6829268292682927\n",
            "f1_score 0.6781400966183575\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.65      0.80      0.72       123\n",
            "           P       0.74      0.56      0.64       123\n",
            "\n",
            "    accuracy                           0.68       246\n",
            "   macro avg       0.69      0.68      0.68       246\n",
            "weighted avg       0.69      0.68      0.68       246\n",
            "\n",
            "@dw_espanol: Lo m치s triste de la #pandemia del #coronavirus son la cantidad de fallecidos N\n",
            "@dw_espanol: Todos los adultos mayores al fin vacunados!!! P\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}